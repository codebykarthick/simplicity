import os
from contextlib import asynccontextmanager

from fastapi import FastAPI, Query
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles

from dto.request import QueryRequest
from dto.response import Abstract, QueryResponse
from utils.config import CONFIG, load_config
from utils.logger import logger, setup_logger


@asynccontextmanager
async def lifespan(server: FastAPI):
    """Manual lifecycle hook to setup configuration before listening for requests.

    Args:
        server (FastAPI): The instance of FastAPI (Not used here).
    """
    load_config()
    setup_logger()
    yield

server = FastAPI(lifespan=lifespan)

# Mount frontend folder as static files
frontend_path = os.path.join(os.path.dirname(__file__), "ui")
server.mount("/static", StaticFiles(directory=frontend_path), name="static")


@server.get("/")
def serve_index() -> FileResponse:
    """Server the simple HTML UI on which user posts the query.

    Returns:
        FileResponse: Standard FastAPI response of the HTML file.
    """
    return FileResponse(os.path.join(frontend_path, "index.html"))


@server.post("/ask")
def handle_query(request: QueryRequest, limit: int = Query(10, ge=1, le=25, description="Max number of abstracts to process for the query")) -> QueryResponse:
    """Endpoint to handle user query for the actual RAG flow.

    Args:
        request (QueryRequest): The query from the user to generate content on.
        limit (int): The maximum number of abstracts to process, summarise and cite in response.

    Returns:
        QueryResponse: The structured response generated by the server.
    """
    logger.info(f"Received request: {request}")
    return QueryResponse(
        summary="Universal Transformers introduce recurrence over depth to improve generalization in sequence tasks [UT]. We can use different algorithms that involve transforms to solve a reinforcement problem. [TR].",
        abstracts=[
            Abstract(
                id="UT",
                title="Universal Transformers",
                authors=[
                    "Mostafa Dehghani", "Stephan Gouws", "Oriol Vinyals",
                    "Jakob Uszkoreit", "≈Åukasz Kaiser"
                ],
                year="2019",
                categories=["cs.CL"],
                abstract=(
                    "Universal Transformers generalize the standard Transformer by introducing recurrence "
                    "over depth, allowing dynamic halting per token and achieving better generalization on "
                    "language and algorithmic tasks. They combine the parallelism of Transformers with the "
                    "inductive bias of RNNs and achieve state-of-the-art results on tasks like bAbI and LAMBADA."
                ),
                pdf_url="https://arxiv.org/pdf/1807.03819"
            ),
            Abstract(
                id="TR",
                title="Transformers in Reinforcement Learning: A Survey",
                authors=[
                    "Pranav Agarwal", "Aamer Abdul Rahman", "Pierre-Luc St-Charles", "Simon J.D. Prince", "Samira Ebrahimi Kahou"
                ],
                year="2023",
                categories=["cs.LG"],
                abstract=(
                    "Transformers have significantly impacted domains like natural language processing, computer vision, and"
                    "robotics, where they improve performance compared to other neural networks. This survey explores how"
                    "transformers are used in reinforcement learning (RL), where they are seen as a promising solution for address-"
                    "ing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability."
                ),
                pdf_url="https://arxiv.org/pdf/2307.05979"
            ),
        ][:limit]
    )
